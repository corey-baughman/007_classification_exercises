{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa888fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import env\n",
    "import acquire as acq\n",
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "import os\n",
    "\n",
    "def prep_iris():\n",
    "    '''\n",
    "    function takes in data from aquire.get_titanic_data(),\n",
    "    applies preparatory steps to the dataset, then splits\n",
    "    the dataset into train, validate, and test groups.\n",
    "    '''\n",
    "    iris_df = acq.get_iris_data()\n",
    "    iris_df.drop(['species_id'], axis=1, inplace=True)\n",
    "    iris_df.rename(columns={'species_name' : 'species'}, inplace=True)\n",
    "    dummy_df = pd.get_dummies(iris_df.species, dummy_na=False, drop_first=True)\n",
    "    iris_df = pd.concat([iris_df, dummy_df], axis=1)\n",
    "    return iris_df\n",
    "\n",
    "\n",
    "def prep_titanic(df):\n",
    "    '''\n",
    "    This function will drop any duplicate observations, \n",
    "    drop ['deck', 'embarked', 'class', 'age'], fill missing embark_town with 'Southampton'\n",
    "    and create dummy vars from sex and embark_town. \n",
    "    '''\n",
    "    df = df.drop_duplicates()\n",
    "    df = df.drop(columns=['deck', 'embarked', 'class', 'age'])\n",
    "    df['embark_town'] = df.embark_town.fillna(value='Southampton')\n",
    "    dummy_df = pd.get_dummies(df[['sex', 'embark_town']], drop_first=True)\n",
    "    df = pd.concat([df, dummy_df], axis=1)\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def split_data(df, stratify_target='target_col_name'):\n",
    "    '''\n",
    "    take in a DataFrame and return train, validate, and test DataFrames; \n",
    "    stratify on target column name. Return train, validate, test DataFrames.\n",
    "    '''\n",
    "    train_validate, test = train_test_split(df, test_size=.2, \n",
    "                                            random_state=9751, stratify=df[stratify_target])\n",
    "    train, validate = train_test_split(train_validate, \n",
    "                                       test_size=.3, \n",
    "                                       random_state=9751, \n",
    "                                       stratify=train_validate[stratify_target])\n",
    "    return train, validate, test\n",
    "\n",
    "\n",
    "Useful Prep Code\n",
    "# narrow down the dataframe to just object datatypes\n",
    "df.select_dtypes(include='object').head()\n",
    "# it looks like embarked and embark_town are the same deal\n",
    "# let's see if thats accurate\n",
    "\n",
    "# what about those nulls???\n",
    "# using boolean masking -> which info is gone from which column?\n",
    "df.isna().sum()[df.isna().sum() > 0]\n",
    "df.isna().sum()[df.isna().sum() > 0] / len(df) # as percents of cols\n",
    "\n",
    "# we can fill the null values in embark_town with the most common\n",
    "# value (southhampton) b y using a fillna()\n",
    "# we can reassign df['embark_town'] to this, or use an inplace=True\n",
    "# NOTE!!! inplace=True changes the function to RETURN A NONETYPE\n",
    "df.embark_town.fillna('Southampton',inplace=True)\n",
    "\n",
    "# examining the distributions\n",
    "# for every column present inside of df:\n",
    "# check if its not an object,\n",
    "# otherwise (which means its a number)\n",
    "# give me that histogram\n",
    "for col in df:\n",
    "    if df[col].dtype != 'O':\n",
    "        plt.hist(df[col])\n",
    "        plt.title(f'Distribution of {col} on the Titanic')\n",
    "        plt.show()\n",
    "        \n",
    "# Check out distributions of numeric columns.\n",
    "num_cols = df.columns[[df[col].dtype == 'int64' for col in df.columns]]\n",
    "for col in num_cols:\n",
    "    plt.hist(df[col])\n",
    "    plt.title(col)\n",
    "    plt.show()\n",
    "    \n",
    "# Use .describe with object columns.\n",
    "\n",
    "obj_cols = df.columns[[df[col].dtype == 'O' for col in df.columns]]\n",
    "for col in obj_cols:\n",
    "    print(df[col].value_counts())\n",
    "    print(df[col].value_counts(normalize=True, dropna=False))\n",
    "    print('----------------------')\n",
    "\n",
    "    # Create bins for fare using .value_counts.\n",
    "# Using sort = false will sort by bin values as opposed to the frequency counts.\n",
    "\n",
    "df.fare.value_counts(bins=5, sort=False)\n",
    "\n",
    "# Find columns with missing values and the total of missing values.\n",
    "\n",
    "missing = df.isnull().sum()\n",
    "missing[missing > 0]\n",
    "\n",
    "# Drop duplicates...run just in case; reassign and check the shape of my data.\n",
    "\n",
    "df = df.drop_duplicates()\n",
    "df.shape\n",
    "\n",
    "# Drop columns with too many missing values for now and reassign; check the shape of my data.\n",
    "\n",
    "cols_to_drop = ['deck', 'embarked', 'class', 'age']\n",
    "df = df.drop(columns=cols_to_drop)\n",
    "df.shape\n",
    "\n",
    "# Run .fillna() on the entire df.\n",
    "\n",
    "df['embark_town'] = df.embark_town.fillna(value='Southampton')\n",
    "# Validate that missing values in embark_town have been handled.\n",
    "\n",
    "df.embark_town.isna().sum()\n",
    "\n",
    "# Using drop_first leaves sex_male, embark_town_Queenstown, and embark_town_Southampton.\n",
    "\n",
    "dummy_df = pd.get_dummies(df[['sex','embark_town']], dummy_na=False, drop_first=[True, True])\n",
    "dummy_df.head()\n",
    "\n",
    "# Concatenate the dummy_df dataframe above with the original df and validate.\n",
    "\n",
    "df = pd.concat([df, dummy_df], axis=1)\n",
    "df.head(1)\n",
    "\n",
    "\n",
    "        \n",
    "Should I do this on the full dataset or on the train sample?\n",
    "this: the action, method, function, step you are about to take on your data.\n",
    "\n",
    "'''Are you comparing, looking at the relationship or summary stats or \n",
    "visualizations with 2+ variables?\n",
    "Are you using an sklearn method?\n",
    "Are you moving into the explore stage of the pipeline?\n",
    "If ONE or more of these is yes, then you should be doing it on your train sample. \n",
    "If ALL are no, then the entire dataset is fine.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
